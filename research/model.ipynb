{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8bef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data Manipulation Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import Data Visualization Libararies\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,RobustScaler,LabelEncoder\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier,BaggingClassifier,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a693f758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>Title</th>\n",
       "      <th>AgeGroup</th>\n",
       "      <th>CabinDeck</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>43.2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>23.01</td>\n",
       "      <td>S</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>Miss</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>PC 27225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>21.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>157.46</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr</td>\n",
       "      <td>YoungAdult</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SC 257787</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>47.2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>131.88</td>\n",
       "      <td>S</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>CA 147316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>15.7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.42</td>\n",
       "      <td>S</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Dr</td>\n",
       "      <td>Teen</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>PC 710570</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>49.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>54.24</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Miss</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>PC 620176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass     Sex   Age  SibSp  Parch    Fare Embarked  \\\n",
       "0            1       3    male  43.2      4      3   23.01        S   \n",
       "1            2       3    male  21.4      2      0  157.46        C   \n",
       "2            3       3  female  47.2      1      4  131.88        S   \n",
       "3            4       1    male  15.7      0      4    3.42        S   \n",
       "4            5       1    male  49.6      2      0   54.24        S   \n",
       "\n",
       "   FamilySize  IsAlone Title    AgeGroup CabinDeck     Ticket  Survived  \n",
       "0           8        0  Miss       Adult   Unknown   PC 27225         0  \n",
       "1           3        0    Mr  YoungAdult   Unknown  SC 257787         1  \n",
       "2           6        0    Mr       Adult   Unknown  CA 147316         1  \n",
       "3           5        0    Dr        Teen   Unknown  PC 710570         0  \n",
       "4           3        0  Miss       Adult   Unknown  PC 620176         0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Ingestion\n",
    "\n",
    "df = pd.read_csv(r'C:\\TitanicPeopleSurvival_PredictionModel\\data\\raw\\Titanic_Dataset.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e653a806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  15000 non-null  int64  \n",
      " 1   Pclass       15000 non-null  int64  \n",
      " 2   Sex          15000 non-null  object \n",
      " 3   Age          15000 non-null  float64\n",
      " 4   SibSp        15000 non-null  int64  \n",
      " 5   Parch        15000 non-null  int64  \n",
      " 6   Fare         15000 non-null  float64\n",
      " 7   Embarked     15000 non-null  object \n",
      " 8   FamilySize   15000 non-null  int64  \n",
      " 9   IsAlone      15000 non-null  int64  \n",
      " 10  Title        15000 non-null  object \n",
      " 11  AgeGroup     14705 non-null  object \n",
      " 12  CabinDeck    15000 non-null  object \n",
      " 13  Ticket       15000 non-null  object \n",
      " 14  Survived     15000 non-null  int64  \n",
      "dtypes: float64(2), int64(7), object(6)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Data Information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e81d33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Unnecessary Columns\n",
    "df.drop(columns = ['PassengerId','CabinDeck','Ticket','Title','AgeGroup'],inplace=True,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "327effb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      Feature  Count  Maximum  Minimum       Mean     Q1      Q2       Q3  \\\n",
       " 0      Pclass  15000     3.00      1.0   2.320933   2.00   3.000   3.0000   \n",
       " 1         Age  15000    78.50      0.0  29.184020  19.70  29.100  38.5000   \n",
       " 2       SibSp  15000     5.00      0.0   2.486067   1.00   2.000   4.0000   \n",
       " 3       Parch  15000     4.00      0.0   2.013600   1.00   2.000   3.0000   \n",
       " 4        Fare  15000   375.72      0.0  31.583209   8.87  21.985  43.6925   \n",
       " 5  FamilySize  15000    10.00      1.0   5.499667   4.00   5.000   7.0000   \n",
       " 6     IsAlone  15000     1.00      0.0   0.034200   0.00   0.000   0.0000   \n",
       " \n",
       "        IQR  Standard Deviation  Skewness   Kurtosis  \n",
       " 0   1.0000            0.829044 -0.656178  -1.232581  \n",
       " 1  18.8000           13.686311  0.079937  -0.263259  \n",
       " 2   3.0000            1.716781  0.007059  -1.280820  \n",
       " 3   2.0000            1.416551 -0.008539  -1.305198  \n",
       " 4  34.8225           31.843415  2.069897   6.877369  \n",
       " 5   3.0000            2.220494  0.013744  -0.642678  \n",
       " 6   0.0000            0.181749  5.126444  24.283671  ,\n",
       " [OrderedDict([('Features', 'Embarked'),\n",
       "               ('Count', np.int64(15000)),\n",
       "               ('Unique Count', 3),\n",
       "               ('Value Counts',\n",
       "                Embarked\n",
       "                S    10813\n",
       "                C     2855\n",
       "                Q     1332\n",
       "                Name: count, dtype: int64),\n",
       "               ('Mode',\n",
       "                0    S\n",
       "                Name: Embarked, dtype: object)])])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data exploration\n",
    "# Handling Leakage\n",
    "'''\n",
    "1. Split the data into X and y\n",
    "2. Use Train Test Split\n",
    "3. Using LabelEncoder\n",
    "4. Using Scaling Technique\n",
    "5. Using SMOT Technique for data balance\n",
    "'''\n",
    "# Split the Data into X and y\n",
    "X = df.drop(columns='Survived',axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# Use Train Test Split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,\n",
    "                                                 test_size=0.3,\n",
    "                                                 random_state=1)\n",
    "\n",
    "# Segregate the Numerical and Categorical Columns\n",
    "numerical_col = X_train.select_dtypes(exclude='object').columns\n",
    "categorical_col = X_train.select_dtypes(include='object').columns\n",
    "\n",
    "numerical_stats = []\n",
    "from collections import OrderedDict\n",
    "for i in numerical_col:\n",
    "    num_stats = OrderedDict({\n",
    "        \"Feature\": i,\n",
    "        \"Count\": df[i].count(),\n",
    "        \"Maximum\": df[i].max(),\n",
    "        \"Minimum\": df[i].min(),\n",
    "        \"Mean\": df[i].mean(),\n",
    "        \"Q1\": df[i].quantile(0.25),\n",
    "        \"Q2\": df[i].quantile(0.50),\n",
    "        \"Q3\": df[i].quantile(0.75),\n",
    "        \"IQR\": df[i].quantile(0.75) - df[i].quantile(0.25),\n",
    "        \"Standard Deviation\": df[i].std(),\n",
    "        \"Skewness\": df[i].skew(),\n",
    "        \"Kurtosis\": df[i].kurt()\n",
    "    })\n",
    "\n",
    "    numerical_stats.append(num_stats)\n",
    "    numerical_stats_report = pd.DataFrame(numerical_stats)\n",
    "\n",
    "categorical_stats = []\n",
    "for i in categorical_col:\n",
    "    cat_stats = OrderedDict({\n",
    "        \"Features\": i,\n",
    "        \"Count\": df[i].count(),\n",
    "        \"Unique Count\": df[i].nunique(),\n",
    "        \"Value Counts\": df[i].value_counts(),\n",
    "        \"Mode\": df[i].mode()\n",
    "    })\n",
    "\n",
    "categorical_stats.append(cat_stats)\n",
    "categorical_stats_report = pd.DataFrame(categorical_stats)\n",
    "\n",
    "numerical_stats_report, categorical_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d92dff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Label Encoding on Categorical Columns\n",
    "le = LabelEncoder()\n",
    "for i in categorical_col:\n",
    "    X_train[i] = le.fit_transform(X_train[i])  # Seen Data\n",
    "    X_test[i] = le.transform(X_test[i])           # Unseen Data\n",
    "\n",
    "# Using Scaling techiques on Numerical Columns\n",
    "sc = RobustScaler()\n",
    "X_train[numerical_col] = sc.fit_transform(X_train[numerical_col]) # Seen data\n",
    "X_test[numerical_col] = sc.transform(X_test[numerical_col]) # Unseen Data\n",
    "\n",
    "# Using SMOTE\n",
    "smote = SMOTE(random_state=1)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train) # Seen data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f93fcd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.44      0.52      2876\n",
      "           1       0.36      0.55      0.43      1624\n",
      "\n",
      "    accuracy                           0.48      4500\n",
      "   macro avg       0.50      0.50      0.48      4500\n",
      "weighted avg       0.54      0.48      0.49      4500\n",
      "\n",
      "Confusion Matix\n",
      "[[1279 1597]\n",
      " [ 731  893]]\n",
      "--------------------------------------------------\n",
      "Model: DecisionTree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.58      0.61      2876\n",
      "           1       0.37      0.44      0.40      1624\n",
      "\n",
      "    accuracy                           0.53      4500\n",
      "   macro avg       0.51      0.51      0.51      4500\n",
      "weighted avg       0.55      0.53      0.54      4500\n",
      "\n",
      "Confusion Matix\n",
      "[[1669 1207]\n",
      " [ 915  709]]\n",
      "--------------------------------------------------\n",
      "Model: RandomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.69      0.66      2876\n",
      "           1       0.37      0.32      0.34      1624\n",
      "\n",
      "    accuracy                           0.56      4500\n",
      "   macro avg       0.50      0.50      0.50      4500\n",
      "weighted avg       0.54      0.56      0.55      4500\n",
      "\n",
      "Confusion Matix\n",
      "[[1978  898]\n",
      " [1103  521]]\n",
      "--------------------------------------------------\n",
      "Model: AdaBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.42      0.51      2876\n",
      "           1       0.35      0.55      0.43      1624\n",
      "\n",
      "    accuracy                           0.47      4500\n",
      "   macro avg       0.49      0.49      0.47      4500\n",
      "weighted avg       0.53      0.47      0.48      4500\n",
      "\n",
      "Confusion Matix\n",
      "[[1221 1655]\n",
      " [ 724  900]]\n",
      "--------------------------------------------------\n",
      "Model: Bagging\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.71      0.68      2876\n",
      "           1       0.39      0.33      0.36      1624\n",
      "\n",
      "    accuracy                           0.57      4500\n",
      "   macro avg       0.52      0.52      0.52      4500\n",
      "weighted avg       0.56      0.57      0.57      4500\n",
      "\n",
      "Confusion Matix\n",
      "[[2041  835]\n",
      " [1080  544]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(),\n",
    "    \"RandomForest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"Bagging\": BaggingClassifier() \n",
    "}\n",
    "\n",
    "for model_name , model in models.items():\n",
    "    model.fit(X_train,y_train) # Seen Data\n",
    "    y_pred = model.predict(X_test) # Unseen data\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Confusion Matix\")\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"-\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TitanicPeopleSurvival_PredictionModel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
